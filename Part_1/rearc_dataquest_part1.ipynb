{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12760ac0",
   "metadata": {},
   "source": [
    "# Rearc Data Quest - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c16b6",
   "metadata": {},
   "source": [
    "## Install dependencies into the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3368ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade pip\n",
    "%pip install --quiet boto3 requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17aa70c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5b92d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: s3://rearc-dataquest-harpreet/part1/bls/pr/ | Region: us-east-2\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "BUCKET_NAME = 'rearc-dataquest-harpreet'\n",
    "AWS_REGION  = 'us-east-2'\n",
    "S3_PREFIX   = 'part1/bls/pr/'\n",
    "DEFAULT_SOURCE = 'https://download.bls.gov/pub/time.series/pr/'\n",
    "DELETE_MISSING_IN_S3 = True\n",
    "DRY_RUN = False\n",
    "RATE_LIMIT_SECONDS = 0.2\n",
    "MAX_RETRIES = 3\n",
    "print(f'Bucket: s3://{BUCKET_NAME}/{S3_PREFIX} | Region: {AWS_REGION}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985cde9b",
   "metadata": {},
   "source": [
    "## Imports & hardened HTTP session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95e5360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP session ready; S3 client will be created lazily by get_s3(). Cell finished.\n"
     ]
    }
   ],
   "source": [
    "# Imports & hardened HTTP session (non-blocking)\n",
    "import os, argparse, hashlib, json, random, sys, time, re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- IMPORTANT: prevent boto3 from hanging while probing EC2 metadata (IMDS) for creds\n",
    "os.environ.setdefault(\"AWS_EC2_METADATA_DISABLED\", \"true\")\n",
    "\n",
    "UA = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Rearc-DataQuest/1.0 (+https://rearc.io)\"\n",
    "\n",
    "def build_http_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": UA,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    })\n",
    "    retry = Retry(\n",
    "        total=5,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\", \"HEAD\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "HTTP = build_http_session()\n",
    "\n",
    "# --- Defer S3 client creation until needed (avoids any credential probing in this cell)\n",
    "def get_s3():\n",
    "    # You can also set profile_name=... if you want to force a specific profile\n",
    "    cfg = Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"},\n",
    "                 connect_timeout=5, read_timeout=20)\n",
    "    session = boto3.Session(region_name=AWS_REGION)\n",
    "    return session.client(\"s3\", config=cfg)\n",
    "\n",
    "print(\"HTTP session ready; S3 client will be created lazily by get_s3(). Cell finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1ba69",
   "metadata": {},
   "source": [
    "## Directory discovery with fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e455f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FALLBACK_FILENAMES = [\n",
    "    'pr.class','pr.contacts','pr.data.0.Current','pr.data.1.AllData',\n",
    "    'pr.duration','pr.footnote','pr.measure','pr.period',\n",
    "    'pr.seasonal','pr.sector','pr.series','pr.txt'\n",
    "]\n",
    "\n",
    "def robust_list_files(index_url: str):\n",
    "    try:\n",
    "        resp = HTTP.get(index_url, timeout=(10,30))\n",
    "        if resp.status_code == 403:\n",
    "            raise RuntimeError('403 on index')\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "        hrefs = re.findall(r'href\\s*=\\s*[\\'\\\"][^\\'\\\"]+[\\'\\\"]', html, flags=re.IGNORECASE)\n",
    "        keep = []\n",
    "        for m in re.finditer(r'href\\s*=\\s*[\\'\\\"]([^\\'\\\"]+)[\\'\\\"]', html, flags=re.IGNORECASE):\n",
    "            h = m.group(1)\n",
    "            if h.startswith('?') or h.endswith('/'):\n",
    "                continue\n",
    "            parsed = urlparse(h)\n",
    "            segment = (parsed.path or h).split('/')[-1].strip()\n",
    "            if segment.startswith('./'):\n",
    "                segment = segment[2:]\n",
    "            if segment.lower().startswith('pr.'):\n",
    "                keep.append((segment, urljoin(index_url, h)))\n",
    "        print(f'Parsed {len(hrefs)} hrefs; keeping {len(keep)} pr.* files')\n",
    "        if keep:\n",
    "            for name, u in keep[:10]:\n",
    "                print(' -', name, '->', u)\n",
    "            return sorted(keep)\n",
    "        print('Index parsed but empty; falling back to known filenames…')\n",
    "        raise RuntimeError('empty-parse')\n",
    "    except Exception as e:\n",
    "        print(f'Directory listing unavailable ({e}); using fallback list.')\n",
    "        keep = [(name, urljoin(index_url, name)) for name in FALLBACK_FILENAMES]\n",
    "        for name, u in keep:\n",
    "            print(' - (fallback)', name, '->', u)\n",
    "        return keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892c392",
   "metadata": {},
   "source": [
    "## HTTP download with backoff & polite rate limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37723f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def http_get_with_backoff(url: str, rate_limit_sec: float = RATE_LIMIT_SECONDS, max_retries: int = MAX_RETRIES) -> bytes:\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            r = HTTP.get(url, timeout=(10,60))\n",
    "            if r.status_code == 403:\n",
    "                raise RuntimeError('403 from source; try again later.')\n",
    "            r.raise_for_status()\n",
    "            time.sleep(rate_limit_sec)\n",
    "            return r.content\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "            backoff = rate_limit_sec * (2 ** (attempt-1)) * (1 + random.random()*0.2)\n",
    "            print(f\"Transient error '{e}', retrying in {backoff:.2f}s … ({attempt}/{max_retries})\")\n",
    "            time.sleep(backoff)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651004e",
   "metadata": {},
   "source": [
    "## S3 helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91887d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sha256_bytes(b: bytes) -> str:\n",
    "    import hashlib\n",
    "    return hashlib.sha256(b).hexdigest()\n",
    "\n",
    "def s3_key_for(prefix: str, name: str) -> str:\n",
    "    return f'{prefix}{name}' if prefix else name\n",
    "\n",
    "def head_sha256(s3_client, bucket: str, key: str):\n",
    "    try:\n",
    "        resp = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        return resp.get('Metadata', {}).get('sha256')\n",
    "    except ClientError as e:\n",
    "        code = e.response.get('Error', {}).get('Code')\n",
    "        if code in ('404','NoSuchKey','NotFound'):\n",
    "            return None\n",
    "        raise\n",
    "\n",
    "def put_with_metadata(s3_client, bucket: str, key: str, body: bytes, sha256: str, source_url: str, dry_run: bool):\n",
    "    if dry_run:\n",
    "        return 'would-put'\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=body,\n",
    "        ContentType='text/plain',\n",
    "        Metadata={'sha256': sha256, 'source_url': source_url},\n",
    "        ServerSideEncryption='AES256',\n",
    "    )\n",
    "    return 'put'\n",
    "\n",
    "def list_keys(s3_client, bucket: str, prefix: str):\n",
    "    keys = set()\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            keys.add(obj['Key'])\n",
    "    return keys\n",
    "\n",
    "def delete_key(s3_client, bucket: str, key: str, dry_run: bool):\n",
    "    if dry_run:\n",
    "        return 'would-delete'\n",
    "    s3_client.delete_object(Bucket=bucket, Key=key)\n",
    "    return 'deleted'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68da3d3d",
   "metadata": {},
   "source": [
    "## Core sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cd2b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_bls_to_s3(bucket: str, region: str, prefix: str,\n",
    "                   source_url: str = DEFAULT_SOURCE,\n",
    "                   delete_missing: bool = True,\n",
    "                   dry_run: bool = False,\n",
    "                   rate_limit_sec: float = RATE_LIMIT_SECONDS,\n",
    "                   max_retries: int = MAX_RETRIES) -> dict:\n",
    "    session = boto3.Session(region_name=region)\n",
    "    s3_client = session.client('s3')\n",
    "    print('Discovering source files…')\n",
    "    src_files = robust_list_files(source_url)\n",
    "    print(f'Found {len(src_files)} files at source.')\n",
    "    before = list_keys(s3_client, bucket, prefix)\n",
    "    seen = set()\n",
    "    results = []\n",
    "    for name, url in tqdm(src_files, desc='Syncing', leave=False):\n",
    "        key = s3_key_for(prefix, name)\n",
    "        try:\n",
    "            b = http_get_with_backoff(url, rate_limit_sec, max_retries)\n",
    "            h = sha256_bytes(b)\n",
    "            existing = head_sha256(s3_client, bucket, key)\n",
    "            if existing == h:\n",
    "                action = 'skipped'\n",
    "            else:\n",
    "                action = put_with_metadata(s3_client, bucket, key, b, h, url, dry_run)\n",
    "            results.append({'name': name, 'key': key, 'action': action, 'sha256': h})\n",
    "            seen.add(key)\n",
    "        except Exception as e:\n",
    "            results.append({'name': name, 'key': key, 'action': 'error', 'error': str(e)})\n",
    "    removed = []\n",
    "    if delete_missing:\n",
    "        to_delete = sorted(before - seen)\n",
    "        for key in to_delete:\n",
    "            removed.append({'key': key, 'action': delete_key(s3_client, bucket, key, dry_run)})\n",
    "    return {'synced': results, 'removed': removed}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70dcf58",
   "metadata": {},
   "source": [
    "## Run the sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e662e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering source files…\n",
      "Parsed 13 hrefs; keeping 12 pr.* files\n",
      " - pr.class -> https://download.bls.gov/pub/time.series/pr/pr.class\n",
      " - pr.contacts -> https://download.bls.gov/pub/time.series/pr/pr.contacts\n",
      " - pr.data.0.Current -> https://download.bls.gov/pub/time.series/pr/pr.data.0.Current\n",
      " - pr.data.1.AllData -> https://download.bls.gov/pub/time.series/pr/pr.data.1.AllData\n",
      " - pr.duration -> https://download.bls.gov/pub/time.series/pr/pr.duration\n",
      " - pr.footnote -> https://download.bls.gov/pub/time.series/pr/pr.footnote\n",
      " - pr.measure -> https://download.bls.gov/pub/time.series/pr/pr.measure\n",
      " - pr.period -> https://download.bls.gov/pub/time.series/pr/pr.period\n",
      " - pr.seasonal -> https://download.bls.gov/pub/time.series/pr/pr.seasonal\n",
      " - pr.sector -> https://download.bls.gov/pub/time.series/pr/pr.sector\n",
      "Found 12 files at source.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sync Summary ===\n",
      "Uploaded/Updated: 12\n",
      "Skipped         : 0\n",
      "Errors          : 0\n",
      "Removed        : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "summary = sync_bls_to_s3(\n",
    "    bucket=BUCKET_NAME,\n",
    "    region=AWS_REGION,\n",
    "    prefix=S3_PREFIX,\n",
    "    source_url=DEFAULT_SOURCE,\n",
    "    delete_missing=DELETE_MISSING_IN_S3,\n",
    "    dry_run=DRY_RUN,\n",
    "    rate_limit_sec=RATE_LIMIT_SECONDS,\n",
    "    max_retries=MAX_RETRIES,\n",
    ")\n",
    "\n",
    "uploaded = sum(1 for x in summary['synced'] if x['action'] == 'put')\n",
    "skipped  = sum(1 for x in summary['synced'] if x['action'] == 'skipped')\n",
    "errors   = [x for x in summary['synced'] if x['action'] == 'error']\n",
    "print('\\n=== Sync Summary ===')\n",
    "print('Uploaded/Updated:', uploaded)\n",
    "print('Skipped         :', skipped)\n",
    "print('Errors          :', len(errors))\n",
    "if errors[:3]:\n",
    "    import json as _json\n",
    "    print('Sample error   :', _json.dumps(errors[:3], indent=2)[:800])\n",
    "print('Removed        :', len(summary['removed']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970eee90",
   "metadata": {},
   "source": [
    "## Verify in S3 (programmatic check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999c14a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyCount: 12\n",
      " - part1/bls/pr/pr.class 102\n",
      " - part1/bls/pr/pr.contacts 562\n",
      " - part1/bls/pr/pr.data.0.Current 1564284\n",
      " - part1/bls/pr/pr.data.1.AllData 3187878\n",
      " - part1/bls/pr/pr.duration 176\n",
      " - part1/bls/pr/pr.footnote 40\n",
      " - part1/bls/pr/pr.measure 745\n",
      " - part1/bls/pr/pr.period 146\n",
      " - part1/bls/pr/pr.seasonal 79\n",
      " - part1/bls/pr/pr.sector 263\n",
      " - part1/bls/pr/pr.series 15657\n",
      " - part1/bls/pr/pr.txt 18343\n"
     ]
    }
   ],
   "source": [
    "# Lazily create S3 client\n",
    "s3 = get_s3()\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=BUCKET_NAME, Prefix=S3_PREFIX)\n",
    "print('KeyCount:', resp.get('KeyCount', 0))\n",
    "for obj in (resp.get('Contents') or [])[:20]:\n",
    "    print(' -', obj['Key'], obj['Size'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
